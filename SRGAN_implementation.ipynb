{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SRGAN_implementation",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt83ZglQvZzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWFKi29AvrMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvn1eX9j2I0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip DIV2K_train_HR.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Td9l7qFp2Z1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob,os\n",
        "img_dir = \"DIV2K_train_HR\"\n",
        "data_path = os.path.join(img_dir,\"*g\")\n",
        "files = glob.glob(data_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6D274Q820gK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5u3Sp_Jdfi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def loadLRImages(imagelist):\n",
        "    images=[]\n",
        "    for image in (imagelist):\n",
        "        img = cv2.resize(cv2.GaussianBlur(cv2.imread(image),(5,5),cv2.BORDER_DEFAULT),(64,64)) \n",
        "\n",
        "        images.append(img)\n",
        "    return np.array(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52OhZq3Kdf_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadHRImages(imagelist):\n",
        "    images=[]\n",
        "    for image in (imagelist):\n",
        "        img = cv2.resize(cv2.imread(image),(256,256))\n",
        "        images.append(img)\n",
        "    return np.array(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS3IqHFTdfoa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images_LR = loadLRImages(files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq21yT5FdgDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images_HR = loadHRImages(files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWN5ItbBfOu4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_crop(lr_img, hr_img, hr_crop_size=96, scale=2):\n",
        "    lr_crop_size = hr_crop_size // scale\n",
        "    lr_img_shape = tf.shape(lr_img)[:2]\n",
        "\n",
        "    lr_w = tf.random.uniform(shape=(), maxval=lr_img_shape[1] - lr_crop_size + 1, dtype=tf.int32)\n",
        "    lr_h = tf.random.uniform(shape=(), maxval=lr_img_shape[0] - lr_crop_size + 1, dtype=tf.int32)\n",
        "\n",
        "    hr_w = lr_w * scale\n",
        "    hr_h = lr_h * scale\n",
        "\n",
        "    lr_img_cropped = lr_img[lr_h:lr_h + lr_crop_size, lr_w:lr_w + lr_crop_size]\n",
        "    hr_img_cropped = hr_img[hr_h:hr_h + hr_crop_size, hr_w:hr_w + hr_crop_size]\n",
        "\n",
        "    return lr_img_cropped, hr_img_cropped"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2BEGb9SxxAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cropped_lr_images = []\n",
        "cropped_hr_images = []\n",
        "for i in range(800):\n",
        "  for j in range(20):\n",
        "    lr_img,hr_img = random_crop(images_LR[j],images_HR[j])\n",
        "  \n",
        "    cropped_lr_images.append(lr_img)\n",
        "    cropped_hr_images.append(hr_img)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpcclrIfKlid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.asarray(cropped_hr_images).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_HJKqiStQjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOVoNFvJVvX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def B_res_blocks(x,n_filters = 64,training = True):\n",
        "  x_3 = tf.keras.layers.Conv2D(filters=n_filters,kernel_size=(3,3),strides=1,padding = 'same')(x)\n",
        "  if training:\n",
        "    x_3 = tf.keras.layers.BatchNormalization()(x_3)\n",
        "  x_4 = tf.keras.layers.PReLU()(x_3)\n",
        "  x_5 = tf.keras.layers.Conv2D(filters = n_filters,kernel_size=(3,3),strides=1,padding = 'same')(x_4)\n",
        "  if training:\n",
        "    x_5 = tf.keras.layers.BatchNormalization()(x_5)\n",
        "  \n",
        "  out = tf.keras.layers.Add()([x,x_5])\n",
        "  return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_LGYz1yNsYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_sub(input_shape, n_filters = 64,training = True):\n",
        "  input_image = tf.keras.Input(shape = input_shape)\n",
        "  c1 = tf.keras.layers.Conv2D(filters=n_filters,kernel_size=(9,9),strides=1,padding = 'same')(input_image)\n",
        "  c1 = tf.keras.layers.PReLU()(c1)\n",
        "  c2 = B_res_blocks(c1,training)\n",
        "  c3 = B_res_blocks(c2,training)\n",
        "  c4 = B_res_blocks(c3,training)\n",
        "  c5 = B_res_blocks(c4,training)\n",
        "  c6 = B_res_blocks(c5,training)\n",
        "  c7 = tf.keras.layers.Conv2D(filters=n_filters,kernel_size=(9,9),strides=1,padding= 'same')(c6)\n",
        "  if training:\n",
        "    c7 = tf.keras.layers.BatchNormalization()(c7)\n",
        "  c8 = tf.keras.layers.Add()([c1,c7])\n",
        "  c9 = tf.keras.layers.Conv2D(filters=n_filters*4,kernel_size=(3,3),strides=1,padding = 'same')(c8)\n",
        "  c10 = tf.keras.layers.UpSampling2D(size=(2,2))(c9)\n",
        "  c11 = tf.keras.layers.PReLU()(c10)\n",
        "  c12 = tf.keras.layers.Conv2D(filters=n_filters*4,kernel_size=(3,3),strides=1,padding = 'same')(c11)\n",
        "  c13 = tf.keras.layers.UpSampling2D(size=(2,2))(c12)\n",
        "  c14 = tf.keras.layers.PReLU()(c13)\n",
        "  out = tf.keras.layers.Conv2D(filters = 3,kernel_size=(9,9),strides=1,padding = 'same')(c14)\n",
        "  gen = tf.keras.Model(input_image,out)\n",
        "  return gen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7Y_BgsPFUeO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def disc_sub(input_shape,n_filters = 64,training = True):\n",
        "  input_image = tf.keras.Input(shape = input_shape)\n",
        "  u1 = tf.keras.layers.Conv2D(filters = n_filters,kernel_size = (3,3),strides=1,padding = 'same')(input_image)\n",
        "  u2 = tf.keras.layers.LeakyReLU()(u1)\n",
        "  u3 = tf.keras.layers.Conv2D(filters = n_filters,kernel_size = (3,3),strides=2,padding = 'same')(u2)\n",
        "  if training:\n",
        "    u3 = tf.keras.layers.BatchNormalization()(u3)\n",
        "  u4 = tf.keras.layers.LeakyReLU()(u3)\n",
        "  u4 = tf.keras.layers.Conv2D(filters = n_filters*2,kernel_size = (3,3),strides=1,padding = 'same')(u4)\n",
        "  if training:\n",
        "    u4 = tf.keras.layers.BatchNormalization()(u4)\n",
        "  u5 = tf.keras.layers.LeakyReLU()(u4)\n",
        "  u5 = tf.keras.layers.Conv2D(filters = n_filters*2,kernel_size = (3,3),strides=2,padding = 'same')(u5)\n",
        "  if training:\n",
        "    u5 = tf.keras.layers.BatchNormalization()(u5)\n",
        "  u6 = tf.keras.layers.LeakyReLU()(u5)\n",
        "  u6 = tf.keras.layers.Conv2D(filters = n_filters*4,kernel_size = (3,3),strides=1,padding = 'same')(u6)\n",
        "  if training:\n",
        "    u6 = tf.keras.layers.BatchNormalization()(u6)\n",
        "  u7 = tf.keras.layers.LeakyReLU()(u6)\n",
        "  u7 = tf.keras.layers.Conv2D(filters = n_filters*4,kernel_size = (3,3),strides=2,padding = 'same')(u7)\n",
        "  if training:\n",
        "    u7 = tf.keras.layers.BatchNormalization()(u7)\n",
        "  u8 = tf.keras.layers.LeakyReLU()(u7)\n",
        "  u8 = tf.keras.layers.Conv2D(filters = n_filters*8,kernel_size = (3,3),strides=1,padding = 'same')(u8)\n",
        "  if training:\n",
        "    u8 = tf.keras.layers.BatchNormalization()(u8)  \n",
        "  u9 = tf.keras.layers.LeakyReLU()(u8)\n",
        "  u9 = tf.keras.layers.Conv2D(filters = n_filters*8,kernel_size = (3,3),strides=2,padding = 'same')(u9)\n",
        "  if training:\n",
        "    u9 = tf.keras.layers.BatchNormalization()(u9)  \n",
        "  u10 = tf.keras.layers.LeakyReLU()(u9) \n",
        "  u10 = tf.keras.layers.Flatten()(u10)\n",
        "  u10 = tf.keras.layers.Dense(1024)(u10)\n",
        "  u10 = tf.keras.layers.LeakyReLU()(u10)\n",
        "  out = tf.keras.layers.Dense(1,activation=\"sigmoid\")(u10)\n",
        "  disc = tf.keras.Model(input_image,out)\n",
        "  return disc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeM04iqXfCRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x = np.asarray(cropped_lr_images[12000:15200])\n",
        "train_y = np.asarray(cropped_hr_images[12000:15200])\n",
        "test_x = np.asarray(cropped_lr_images[15200:16000])\n",
        "test_y = np.asarray(cropped_hr_images[15200:16000])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sfzrXFYi0W3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(cropped_hr_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBiqcBT3Uyeg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "genet = gen_sub(train_x.shape[1:])\n",
        "disc_hr = disc_sub(train_y.shape[1:])\n",
        "disc_sr = disc_sub((192,192,3))\n",
        "disc_hr.compile(loss=tf.keras.losses.BinaryCrossentropy(),optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.002),metrics=['accuracy'])\n",
        "disc_sr.compile(loss=tf.keras.losses.BinaryCrossentropy(),optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.002),metrics=['accuracy'])\n",
        "\n",
        "frozen = tf.keras.Model(disc_sr.inputs,disc_sr.outputs)\n",
        "frozen.trainable = False\n",
        "noise = tf.keras.Input(shape = (48,48,3))\n",
        "image = genet(noise)\n",
        "logit = frozen(image)\n",
        "srgan = tf.keras.Model(noise,logit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INevhqAxhO9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "srgan.compile(loss =, optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.002),metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LPM2jbOehev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "srgan.compile(loss =tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.002),metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fU-EmdbAB9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvMmDSaH2C07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "batch_size = 10\n",
        "n_critic = train_x.shape[0]//batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwJE8Dc6POTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "srgan = tf.keras.models.load_model(\"srgan_model\")\n",
        "genet = tf.keras.models.load_model(\"gen_sub_model\")\n",
        "disc_hr = tf.keras.models.load_model(\"disc_hr_model\")\n",
        "disc_sr = tf.keras.models.load_model(\"disc_sr_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgeuHNNkhe5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "clip_value = 0.01\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "  for i in range(n_critic):\n",
        "    batch_lr = train_x[i*batch_size:(i+1)*batch_size]\n",
        "    batch_hr = train_y[i*batch_size:(i+1)*batch_size]\n",
        "    \n",
        "    batch_sr = genet.predict(batch_lr)\n",
        "\n",
        "    d_loss_real = disc_hr.train_on_batch(batch_hr,np.ones((batch_size,1)))\n",
        "    d_loss_fake = disc_sr.train_on_batch(batch_sr,np.zeros((batch_size,1)))\n",
        "    d_loss = 0.5 * np.add(d_loss_real,d_loss_fake)\n",
        "    filepath = \"checkpoint\"\n",
        "    disc_hr.trainable = False\n",
        "    disc_sr.trainable = False\n",
        "    print(batch_lr.shape)\n",
        "    g_loss = srgan.train_on_batch(batch_lr,-np.ones((batch_size,1)))\n",
        "    disc_hr.trainable = True\n",
        "    disc_sr.trainable = True    \n",
        "    \n",
        "  print(\"epoch {} and loss {} \".format(epoch,g_loss[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIgTAX7b_zAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "genet.save(\"gen_sub_model\")\n",
        "srgan.save(\"srgan_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__W5-tMbL_L9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "disc_hr.save(\"disc_hr_model\")\n",
        "disc_sr.save(\"disc_hr_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YquFZ5TKIUE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "def compute_psnr(img1, img2):\n",
        "  img1 = img1.astype(np.float64) / 255.\n",
        "  img2 = img2.astype(np.float64) / 255.\n",
        "  mse = np.mean((img1 - img2) ** 2)\n",
        "  if mse == 0:\n",
        "    return \"Same Image\"\n",
        "  return 10 * math.log10(1. / mse)\n",
        "def psnr_test(test_LR,test_HR):\n",
        "  image_ = test_LR\n",
        "  image_gen_ = genet.predict(image_)\n",
        "  \n",
        "  psnr_result = compute_psnr(np.resize(image_gen_,(100,96,96,3)),test_HR)\n",
        "  return psnr_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtpiX2R9GvVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "psnr_test(test_x[:100],test_y[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3U3MxI1IT_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNoCLksQIT6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbpY0OKeIT35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWrvtQ0mPJIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}